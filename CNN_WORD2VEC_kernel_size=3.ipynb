{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_WORD2VEC_kernel_size=3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VZYQ7e_1w-et",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hGDejI5OBbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train ve test verilerini train ve test olarak okuduk\n",
        "train=pd.read_excel(\"clean_tweet_train.xlsx\")\n",
        "test=pd.read_excel(\"clean_tweet_test.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0sel9_CDyCbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = train.append(test, ignore_index=True).fillna(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ZS33PEHOMov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test.dropna(inplace=True)\n",
        "test.reset_index(drop=True,inplace=True)\n",
        "\n",
        "train.dropna(inplace=True)\n",
        "train.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qq2GFUVCONm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train=train.text\n",
        "y_train=train.sentiment\n",
        "x_test=test.text\n",
        "y_test=test.sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soNK9_1ZOUwH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import multiprocessing\n",
        "from sklearn import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wsxCqfGvOi1w",
        "colab_type": "code",
        "outputId": "d6a1eaa7-37f1-4b43-c483-464fb72fdb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xCfwqAyOOmPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Her tweet'i unique bir ID ile etiketliyoruz\n",
        "def labelize_tweets_ug(tweets,label):\n",
        "    result = []\n",
        "    prefix = label\n",
        "    for i, t in zip(tweets.index, tweets):\n",
        "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhQ8H0hdOsvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bütün tweet verilerini topladık -train-test olan text columnlar  toplandı\n",
        "all_x = pd.concat([x_train,x_test])\n",
        "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5K1RER6LO9fs",
        "colab_type": "code",
        "outputId": "6e2abe31-eb5c-40f0-d162-ccbf967b9acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#tweet kelimelerine word2vec cbow yöntemi(sg=0) uygulanıyor,\n",
        "#cümle içindeki current_wod ile predicted word arasındaki mesafe window_size=2\n",
        "#size=100 feature vetörlerin boyutu\n",
        "cores = multiprocessing.cpu_count()\n",
        "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17287/17287 [00:00<00:00, 584417.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-hWmgeh_Owha",
        "colab_type": "code",
        "outputId": "5c402389-f95f-45c0-fdf3-f5440ac118dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "cell_type": "code",
      "source": [
        "#embedding eğitimi yapılıyor\n",
        "for epoch in range(30):\n",
        "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "    model_ug_cbow.alpha -= 0.002\n",
        "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17287/17287 [00:00<00:00, 607240.40it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 908209.85it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 787451.22it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 730048.26it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 990072.01it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 690772.48it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1025601.27it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 801677.65it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 775019.33it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 809048.57it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1021814.47it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 598232.15it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 568316.32it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 581343.72it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1016086.73it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1183323.00it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 534054.18it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1244284.27it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 647603.06it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 881286.11it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 715030.31it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 756865.24it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 676711.53it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 833278.93it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 721203.68it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1127056.62it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1020994.33it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 959099.11it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 751912.61it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 899757.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eyUdWMXs09a5",
        "colab_type": "code",
        "outputId": "47a31c10-7e06-4ace-ea01-9da873b9e9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#daha sonra skip-gram modeli \n",
        "model_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17287/17287 [00:00<00:00, 636835.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vq0pFGLW0-cj",
        "colab_type": "code",
        "outputId": "462a83af-dfdb-446d-9016-bbd6ab0ef854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#kelime vektörlerinin elde edilemesi için skip-gram modeli kullanılıyor\n",
        "for epoch in range(30):\n",
        "    model_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "    model_sg.alpha -= 0.002\n",
        "    model_sg.min_alpha = model_sg.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17287/17287 [00:00<00:00, 808642.54it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 538404.49it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 528206.70it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 711759.43it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 608892.62it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 920921.76it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 827138.18it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 981481.33it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 783646.94it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 578435.85it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 764663.61it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 851290.12it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 798095.03it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 783122.18it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 908403.28it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 902209.06it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 715319.53it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 715969.36it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 734292.04it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 703814.15it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1025398.22it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 866323.36it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 1396136.12it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 731086.17it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 903873.61it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 765980.34it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 915780.65it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 681488.16it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 754565.29it/s]\n",
            "100%|██████████| 17287/17287 [00:00<00:00, 988034.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.9 s, sys: 233 ms, total: 23.1 s\n",
            "Wall time: 25.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kTwGzx731BB5",
        "colab_type": "code",
        "outputId": "d1184438-3eb7-479b-a858-bf04aba9f972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#bu iki yöntemi birleştirdik\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_sg.wv[w])\n",
        "print('Word vektör sayısı:' , len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word vektör sayısı: 12029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4z3mCMIUPiSB",
        "colab_type": "code",
        "outputId": "c167a291-1a86-4e01-bae9-081e21552c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#tokenizer ile  cümleydeki kelimeleri bölüyoruz\n",
        "#her cümlenin sequential gösterimi için text_to_sequences kullanıyoruz\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "\n",
        "print(\"Toplam kelime sayısı-train verisindeki:\",len(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toplam kelime sayısı-train verisindeki: 26543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qWVJU82bPpqi",
        "colab_type": "code",
        "outputId": "cfdbd6b0-3175-4cca-9693-57fd6d0c8b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "length = []\n",
        "for x in corpus['text']:#bütün veri setindeki max kelime saysını bulmak için\n",
        "    length.append(len(x.split()))\n",
        "#padding için en uzun cümledeki kelime sayısını bulduk\n",
        "print(\"max kelime sayısı,cümledeki\")\n",
        "max(length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max kelime sayısı,cümledeki\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "QU3IBK12P4J1",
        "colab_type": "code",
        "outputId": "a3d54e72-d766-4bc7-a94f-f4ccfe93aeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "#train veri setindeki bütün cümleler 35 uzunluğuna çevrildi ,0 padding yapıldı\n",
        "x_train_seq = pad_sequences(sequences, maxlen=35)\n",
        "x_train_seq[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    4,   78, 3207,    1,\n",
              "        3208, 1603],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,  919,    1,   25,   26,   12,  252,    2,\n",
              "          28, 4856],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
              "           7,  300],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    1, 4857,\n",
              "         293,   41],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    1,   54, 2370,  103,  194, 2721,   11,   33,\n",
              "           2, 6519]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "Rh5jxEJLP9wf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test veri setindeki bütün cümleler 35 uzunluğuna çevrildi ,0 padding yapıldı\n",
        "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_test_seq = pad_sequences(sequences_test, maxlen=35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpYvUws7y1dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#elde ettiğimix-z kelime vektörlerinden bir matrix oluşturuyoruz ,embedding layer\n",
        "#için num_words ile training için kullanacağımız most frequent word sayısı belirlendi\n",
        "#200 ise embedding_dimension \n",
        "num_words = 10000\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euLeGQPaQGEi",
        "colab_type": "code",
        "outputId": "0e7c177e-21de-4d57-f69d-117d8d572ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "#üç sınıflı bir veri setimiz olduğu için  3 \n",
        "from keras import utils as np_utils\n",
        "y_test = np_utils.to_categorical(y_test, num_classes= 3)\n",
        "y_train = np_utils.to_categorical(y_train, num_classes= 3)\n",
        "print(\"y_train görünümü:\")\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train görünümü:\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HjSx61YzQM8Y",
        "colab_type": "code",
        "outputId": "b44a7c71-2907-4faa-b20c-7d7d02af6d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(x_train_seq.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test_seq.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13831, 35)\n",
            "(13831, 3)\n",
            "(3456, 35)\n",
            "(3456, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "buMJu83eQU34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_features = 10000#training için most frequent 10.000 kelime kullanılacak\n",
        "maxlen =35#en uzun cümledeki kelime sayısı+2\n",
        "embedding_dims = 200#output vektör size,her cümle 35*200 matrix ile ifade edilecek,ayrıca filter column genişliği\n",
        "filters = 32#dimensionality of the output \n",
        "kernel_size = 3#window size uzunluğu\n",
        "hidden_dims = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqeNeQawQuSU",
        "colab_type": "code",
        "outputId": "ca9a8e23-292c-4d2a-a267-02615863245c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "cell_type": "code",
      "source": [
        "#ilk durumda word embeddingler embedding layerdan elde edildi\n",
        "model = Sequential()\n",
        "#embedding layer ile vocab indexleri embedding dimensions'lara çeviriyor\n",
        "model.add(Embedding(max_features, embedding_dims,input_length=maxlen))\n",
        "#dropout overfitting'i önlemek için kullanıldı\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#konvolüsyon katmanı,stride 1 strides vertically\n",
        "model.add(Conv1D(filters,kernel_size,padding='same',activation='relu',strides=1))\n",
        "\n",
        "# max pooling katmanında ;output dimensiondaki her filtreden max olanı alır,1 boyutlu\n",
        "#bir vektör elde etmek için ,uzunluğu filtre sayısı ile aynıdır\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#  hidden layer\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# output layer aktivasyon 'softmax'\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 35, 200)           2000000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 35, 200)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 35, 32)            19232     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 2,021,539\n",
            "Trainable params: 2,021,539\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L02v2YXXss23",
        "colab_type": "code",
        "outputId": "a40c33c0-4919-4341-f647-dcf0cc61d828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train_seq, y_train, epochs=10, batch_size=32)\n",
        "scores = model.evaluate(x_test_seq, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13831/13831 [==============================] - 9s 624us/step - loss: 0.8984 - acc: 0.5765\n",
            "Epoch 2/10\n",
            "13831/13831 [==============================] - 6s 435us/step - loss: 0.5326 - acc: 0.7924\n",
            "Epoch 3/10\n",
            "13831/13831 [==============================] - 6s 443us/step - loss: 0.2703 - acc: 0.9065\n",
            "Epoch 4/10\n",
            "13831/13831 [==============================] - 6s 434us/step - loss: 0.1510 - acc: 0.9530\n",
            "Epoch 5/10\n",
            "13831/13831 [==============================] - 6s 428us/step - loss: 0.1040 - acc: 0.9706\n",
            "Epoch 6/10\n",
            "13831/13831 [==============================] - 6s 431us/step - loss: 0.0780 - acc: 0.9766\n",
            "Epoch 7/10\n",
            "13831/13831 [==============================] - 6s 424us/step - loss: 0.0677 - acc: 0.9800\n",
            "Epoch 8/10\n",
            "13831/13831 [==============================] - 6s 433us/step - loss: 0.0562 - acc: 0.9825\n",
            "Epoch 9/10\n",
            "13831/13831 [==============================] - 6s 421us/step - loss: 0.0449 - acc: 0.9845\n",
            "Epoch 10/10\n",
            "13831/13831 [==============================] - 5s 364us/step - loss: 0.0412 - acc: 0.9837\n",
            "Accuracy: 66.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8vbcKSQ3f3x",
        "colab_type": "code",
        "outputId": "5b163956-02d0-4c45-d728-00da8e3d8e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "cell_type": "code",
      "source": [
        "#ikinci durumda kelime vektörleri Word2vec ten elde edildi\n",
        "model = Sequential()\n",
        "#embedding layer ile vocab indexleri embedding dimensions'lara çeviriyor\n",
        "model.add(Embedding(max_features, embedding_dims, weights=[embedding_matrix],input_length=maxlen,trainable=False))\n",
        "#dropout overfitting'i önlemek için kullanıldı\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#konvolüsyon katmanı,stride 1 strides vertically\n",
        "model.add(Conv1D(filters,kernel_size,padding='same',activation='relu',strides=1))\n",
        "\n",
        "# max pooling katmanında ;output dimensiondaki her filtreden max olanı alır,1 boyutlu\n",
        "#bir vektör elde etmek için ,uzunluğu filtre sayısı ile aynıdır\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#  hidden layer\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# output layer aktivasyon 'softmax'\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 35, 200)           2000000   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 35, 200)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 35, 32)            19232     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 2,021,539\n",
            "Trainable params: 21,539\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "le57fRRE3gQm",
        "colab_type": "code",
        "outputId": "1a394d58-534a-43fb-e859-bfc9ff175191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train_seq, y_train, epochs=10, batch_size=32)\n",
        "scores = model.evaluate(x_test_seq, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13831/13831 [==============================] - 6s 410us/step - loss: 0.9761 - acc: 0.5413\n",
            "Epoch 2/10\n",
            "13831/13831 [==============================] - 5s 362us/step - loss: 0.8310 - acc: 0.6287\n",
            "Epoch 3/10\n",
            "13831/13831 [==============================] - 5s 364us/step - loss: 0.7910 - acc: 0.6483\n",
            "Epoch 4/10\n",
            "13831/13831 [==============================] - 5s 349us/step - loss: 0.7591 - acc: 0.6646\n",
            "Epoch 5/10\n",
            "13831/13831 [==============================] - 5s 355us/step - loss: 0.7346 - acc: 0.6740\n",
            "Epoch 6/10\n",
            "13831/13831 [==============================] - 5s 361us/step - loss: 0.7144 - acc: 0.6853\n",
            "Epoch 7/10\n",
            "13831/13831 [==============================] - 5s 351us/step - loss: 0.6884 - acc: 0.6971\n",
            "Epoch 8/10\n",
            "13831/13831 [==============================] - 5s 364us/step - loss: 0.6712 - acc: 0.7095\n",
            "Epoch 9/10\n",
            "13831/13831 [==============================] - 5s 354us/step - loss: 0.6563 - acc: 0.7146\n",
            "Epoch 10/10\n",
            "13831/13831 [==============================] - 5s 364us/step - loss: 0.6447 - acc: 0.7157\n",
            "Accuracy: 65.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3mE4vIUU3-GT",
        "colab_type": "code",
        "outputId": "9904820f-7022-4c77-f49a-cede51741f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "cell_type": "code",
      "source": [
        "#üçüncü durumda word2vec kelime vektörleri training sırasında update edildi\n",
        "model = Sequential()\n",
        "#embedding layer ile vocab indexleri embedding dimensions'lara çeviriyor\n",
        "model.add(Embedding(max_features, embedding_dims, weights=[embedding_matrix],input_length=maxlen,trainable=True))\n",
        "#dropout overfitting'i önlemek için kullanıldı\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#konvolüsyon katmanı,stride 1 strides vertically\n",
        "model.add(Conv1D(filters,kernel_size,padding='same',activation='relu',strides=1))\n",
        "\n",
        "# max pooling katmanında ;output dimensiondaki her filtreden max olanı alır,1 boyutlu\n",
        "#bir vektör elde etmek için ,uzunluğu filtre sayısı ile aynıdır\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#  hidden layer\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# output layer aktivasyon 'softmax'\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 35, 200)           2000000   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 35, 200)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 35, 32)            19232     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 2,021,539\n",
            "Trainable params: 2,021,539\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J2NF-fKi4AjW",
        "colab_type": "code",
        "outputId": "cde89e11-b3bb-43be-fbb4-a29db673217d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train_seq, y_train, epochs=10, batch_size=32)\n",
        "scores = model.evaluate(x_test_seq, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13831/13831 [==============================] - 7s 473us/step - loss: 0.9297 - acc: 0.5653\n",
            "Epoch 2/10\n",
            "13831/13831 [==============================] - 3s 246us/step - loss: 0.7812 - acc: 0.6532\n",
            "Epoch 3/10\n",
            "13831/13831 [==============================] - 3s 246us/step - loss: 0.6905 - acc: 0.7010\n",
            "Epoch 4/10\n",
            "13831/13831 [==============================] - 3s 244us/step - loss: 0.6055 - acc: 0.7478\n",
            "Epoch 5/10\n",
            "13831/13831 [==============================] - 3s 246us/step - loss: 0.5312 - acc: 0.7806\n",
            "Epoch 6/10\n",
            "13831/13831 [==============================] - 3s 244us/step - loss: 0.4520 - acc: 0.8193\n",
            "Epoch 7/10\n",
            "13831/13831 [==============================] - 3s 245us/step - loss: 0.3847 - acc: 0.8502\n",
            "Epoch 8/10\n",
            "13831/13831 [==============================] - 3s 244us/step - loss: 0.3308 - acc: 0.8712\n",
            "Epoch 9/10\n",
            "13831/13831 [==============================] - 3s 244us/step - loss: 0.2814 - acc: 0.8905\n",
            "Epoch 10/10\n",
            "13831/13831 [==============================] - 3s 246us/step - loss: 0.2371 - acc: 0.9115\n",
            "Accuracy: 67.85%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}